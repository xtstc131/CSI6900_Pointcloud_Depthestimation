{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objcectron Tutorial: Parsing Annotation Data\n",
    "\n",
    "This tutorial covers how to parse the annotation protobufs and visualize them on an image directly. \n",
    "\n",
    "The annotations are stored in [protocol buffer](https://developers.google.com/protocol-buffers) format. For more information, checkout [Protobuf Basics](https://developers.google.com/protocol-buffers/docs/pythontutorial#compiling-your-protocol-buffers). If you want to re-compile our protos to _pb2.py files, you'll need the [protobuf compiler](http://google.github.io/proto-lens/installing-protoc.html). The annotation protos are stored in objectron/schema folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = '.../annotations/class/batch-i/j.pbdata'\n",
    "video_filename = '.../videos/class/batch-i/j/video.MOV'\n",
    "# Along with the video.MOV file, there is a geometry.pbdata file that contains\n",
    "# the geometry information of the scene (such as camera poses, point-clouds, and surfaces).\n",
    "# There is a copy of this container within each annotation protobuf too.\n",
    "geometry_filename = '.../videos/class/batch-i/j/geometry.pbdata'# a.k.a. AR metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2052/3738250877.py:12: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display,HTML\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "#import box as Box\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from IPython.core.display import display,HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# The annotations are stored in protocol buffer format. \n",
    "from objectron.schema import object_pb2 as object_protocol\n",
    "from objectron.schema import annotation_data_pb2 as annotation_protocol\n",
    "# The AR Metadata captured with each frame in the video\n",
    "from objectron.schema import a_r_capture_metadata_pb2 as ar_metadata_protocol\n",
    "from objectron.dataset import box as Box\n",
    "from objectron.dataset import graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the annotation proto from the file, we can iterate over each field\n",
    "and convert them to numpy arrays or tf.tensors.\n",
    "\n",
    "Notable fields are category, and keypoints. The keypoints contains the box's 9 keypoints.\n",
    "The box is defined as follows:\n",
    "```\n",
    "\n",
    "              x                              x\n",
    "      1 + + + + + + + + 5                 .-------\n",
    "      +\\                +\\                |\\\n",
    "      + \\ y             + \\             z | \\ y\n",
    "      +  \\              +  \\              |  \\\n",
    "      +   3 + + + + + + + + 7\n",
    "    z +   +             +   +\n",
    "      +   +             +   +\n",
    "      +   +     C=0     +   +\n",
    "      +   +             +   +\n",
    "      2 + + + + + + + + 6   +\n",
    "       \\  +              \\  +\n",
    "        \\ +               \\ +\n",
    "         \\+                \\+\n",
    "          4 + + + + + + + + 8\n",
    "```\n",
    "In the **world coordinate** system: $+y$ is up (aligned with gravity), $+z$ is toward the user, $+x$ follows right hand rule. The front face is defined as $+z$ axis on $xy$ plane.  The top face is defined as $+y$ axis on $xz$ plane.\n",
    "\n",
    "Each annotation frame contains the keypoints in 3D in the **camera coordinates**, as well as their 2D-projection + depth in image coordinate. You can also get the box's 9-DoF parameters (rotation, translation, and scale) too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_frame_annotation(sequence, frame_id):\n",
    "  \"\"\"Grab an annotated frame from the sequence.\"\"\"\n",
    "  data = sequence.frame_annotations[frame_id]\n",
    "  object_id = 0\n",
    "  object_keypoints_2d = []\n",
    "  object_keypoints_3d = []\n",
    "  object_rotations = []\n",
    "  object_translations = []\n",
    "  object_scale = []\n",
    "  num_keypoints_per_object = []\n",
    "  object_categories = []\n",
    "  annotation_types = []\n",
    "  # Get the camera for the current frame. We will use the camera to bring\n",
    "  # the object from the world coordinate to the current camera coordinate.\n",
    "  camera = np.array(data.camera.transform).reshape(4, 4)\n",
    "\n",
    "  for obj in sequence.objects:\n",
    "    rotation = np.array(obj.rotation).reshape(3, 3)\n",
    "    translation = np.array(obj.translation)\n",
    "    object_scale.append(np.array(obj.scale))\n",
    "    transformation = np.identity(4)\n",
    "    transformation[:3, :3] = rotation\n",
    "    transformation[:3, 3] = translation\n",
    "    obj_cam = np.matmul(camera, transformation)\n",
    "    object_translations.append(obj_cam[:3, 3])\n",
    "    object_rotations.append(obj_cam[:3, :3])\n",
    "    object_categories.append(obj.category)\n",
    "    annotation_types.append(obj.type)\n",
    "\n",
    "  keypoint_size_list = []\n",
    "  for annotations in data.annotations:\n",
    "    num_keypoints = len(annotations.keypoints)\n",
    "    keypoint_size_list.append(num_keypoints)\n",
    "    for keypoint_id in range(num_keypoints):\n",
    "      keypoint = annotations.keypoints[keypoint_id]\n",
    "      object_keypoints_2d.append(\n",
    "          (keypoint.point_2d.x, keypoint.point_2d.y, keypoint.point_2d.depth))\n",
    "      object_keypoints_3d.append(\n",
    "          (keypoint.point_3d.x, keypoint.point_3d.y, keypoint.point_3d.z))\n",
    "    num_keypoints_per_object.append(num_keypoints)\n",
    "    object_id += 1\n",
    "  return (object_keypoints_2d, object_categories, keypoint_size_list,\n",
    "          annotation_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick hack using ffmpeg to grab a single frame from a video file. This is very slow. We recommend using OpenCV if you want to grab multiple frames from the video file instead of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_frame(video_file, frame_ids):\n",
    "  \"\"\"Grab an image frame from the video file.\"\"\"\n",
    "  frames = []\n",
    "  capture = cv2.VideoCapture(video_file)\n",
    "  height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "  width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  capture.release()\n",
    "  frame_size = width * height * 3\n",
    "\n",
    "  for frame_id in frame_ids:\n",
    "    frame_filter = r'select=\\'eq(n\\,{:d})\\''.format(frame_id)\n",
    "    command = [\n",
    "        'ffmpeg', '-i', video_file, '-f', 'image2pipe', '-vf', frame_filter,\n",
    "        '-pix_fmt', 'rgb24', '-vcodec', 'rawvideo', '-vsync', 'vfr', '-'\n",
    "    ]\n",
    "    pipe = subprocess.Popen(\n",
    "        command, stdout=subprocess.PIPE, bufsize = 151 * frame_size)\n",
    "    current_frame = np.fromstring(\n",
    "        pipe.stdout.read(frame_size), dtype='uint8').reshape(width, height, 3)\n",
    "    pipe.stdout.flush()\n",
    "\n",
    "    frames.append(current_frame)\n",
    "  return frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, grab a frame, load the corresponding annotation file and show the 3D bounding box on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.../annotations/class/batch-i/j.pbdata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m frame_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pb:\n\u001b[1;32m      3\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m annotation_protocol\u001b[38;5;241m.\u001b[39mSequence()\n\u001b[1;32m      4\u001b[0m     sequence\u001b[38;5;241m.\u001b[39mParseFromString(pb\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.../annotations/class/batch-i/j.pbdata'"
     ]
    }
   ],
   "source": [
    "frame_id = 100\n",
    "with open(annotation_file, 'rb') as pb:\n",
    "    sequence = annotation_protocol.Sequence()\n",
    "    sequence.ParseFromString(pb.read())\n",
    "    frame = grab_frame(video_filename, [frame_id])\n",
    "    annotation, cat, num_keypoints, types = get_frame_annotation(sequence, frame_id)\n",
    "    image = graphics.draw_annotation_on_image(frame[0], annotation, num_keypoints)\n",
    "    imgplot = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
